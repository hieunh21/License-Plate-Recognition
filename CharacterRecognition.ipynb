{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2634,
     "status": "ok",
     "timestamp": 1762744602755,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "tqpD1wwotyoI",
    "outputId": "b0690665-1094-468d-841a-06786571f2ed"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ6-vcpqrQQ_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762744602783,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "nLig1L4Immz1",
    "outputId": "583df4fd-f46a-4dbf-a250-67921b3e1d11"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Configuration and Constants\n",
    "IMAGE_SIZE = 28\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "WEIGHT_INIT = 0.05 # Hệ số khởi tạo trọng số ban đầu\n",
    "WEIGHT_DECAY = 1e-4 # Hệ số phạt L2 (regularization)\n",
    "\n",
    "ALPHA_DICT = {\n",
    "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H',\n",
    "    8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'R', 14: 'S', 15: 'T',\n",
    "    16: 'U', 17: 'V', 18: 'X', 19: 'Y', 20: 'Z', 21: '0', 22: '1', 23: '2',\n",
    "    24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: \"Background\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Total Classes: {len(ALPHA_DICT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100349,
     "status": "ok",
     "timestamp": 1762744703141,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "SNIis0k1o2yw",
    "outputId": "f282b2b6-baa3-40ed-ba54-520bffa5505f"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Load and Process Dataset\n",
    "\n",
    "def load_character_data(data_path, image_size=28):\n",
    "    \"\"\"\n",
    "    Load and process character images from directory structure.\n",
    "\n",
    "    Parameters:\n",
    "        data_path: Path to the dataset directory\n",
    "        image_size: Target size for resizing images\n",
    "\n",
    "    Returns:\n",
    "        X_digits, y_digits: Arrays for digit data\n",
    "        X_alphas, y_alphas: Arrays for alpha data\n",
    "    \"\"\"\n",
    "    # Label mappings\n",
    "    digit_labels = {\n",
    "        \"0\": 21, \"1\": 22, \"2\": 23, \"3\": 24, \"4\": 25,\n",
    "        \"5\": 26, \"6\": 27, \"7\": 28, \"8\": 29, \"9\": 30, \"BG\": 31\n",
    "    }\n",
    "\n",
    "    alpha_labels = {\n",
    "        \"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7,\n",
    "        \"K\": 8, \"L\": 9, \"M\": 10, \"N\": 11, \"P\": 12, \"R\": 13, \"S\": 14,\n",
    "        \"T\": 15, \"U\": 16, \"V\": 17, \"X\": 18, \"Y\": 19, \"Z\": 20\n",
    "    }\n",
    "\n",
    "    X_digits, y_digits = [], []\n",
    "    X_alphas, y_alphas = [], []\n",
    "\n",
    "    # Load digit data\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        if folder_name in digit_labels:\n",
    "            label = digit_labels[folder_name]\n",
    "            folder_path = os.path.join(data_path, folder_name)\n",
    "\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (image_size, image_size), cv2.INTER_AREA)\n",
    "                img = img.reshape((image_size, image_size, 1))\n",
    "                X_digits.append(img)\n",
    "                y_digits.append(label)\n",
    "\n",
    "    # Load alpha data\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        if folder_name in alpha_labels:\n",
    "            label = alpha_labels[folder_name]\n",
    "            folder_path = os.path.join(data_path, folder_name)\n",
    "\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (image_size, image_size), cv2.INTER_AREA)\n",
    "                img = img.reshape((image_size, image_size, 1))\n",
    "                X_alphas.append(img)\n",
    "                y_alphas.append(label)\n",
    "\n",
    "    X_digits = np.array(X_digits)\n",
    "    y_digits = np.array(y_digits)\n",
    "    X_alphas = np.array(X_alphas)\n",
    "    y_alphas = np.array(y_alphas)\n",
    "\n",
    "    print(f\"Loaded {len(X_digits)} digit samples\")\n",
    "    print(f\"Loaded {len(X_alphas)} alpha samples\")\n",
    "\n",
    "    return X_digits, y_digits, X_alphas, y_alphas\n",
    "\n",
    "# Example usage:\n",
    "data_path = \"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/characters_thi_giac_may_tinh/char_bien_so\"\n",
    "X_digits, y_digits, X_alphas, y_alphas = load_character_data(data_path, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1762744703286,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "fZtcaypJqCpg",
    "outputId": "129f12f7-26bf-4e3a-d1be-4cc0bf5ce4c8"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Split and Save Dataset\n",
    "\n",
    "def split_and_save_data(X_digits, y_digits, X_alphas, y_alphas,\n",
    "                        output_dir=\"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/characters_thi_giac_may_tinh/char_bien_so\",\n",
    "                        test_size=0.3, val_ratio=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train/val/test sets and save as .npy files.\n",
    "\n",
    "    Parameters:\n",
    "        X_digits, y_digits: Digit dataset arrays\n",
    "        X_alphas, y_alphas: Alpha dataset arrays\n",
    "        output_dir: Directory to save the .npy files\n",
    "        test_size: Combined size of validation + test sets\n",
    "        val_ratio: Ratio to split val/test from the test_size portion\n",
    "        random_state: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all split datasets\n",
    "    \"\"\"\n",
    "    # Split digit data: 70% train, 15% val, 15% test\n",
    "    X_train_digits, X_temp_digits, y_train_digits, y_temp_digits = train_test_split(\n",
    "        X_digits, y_digits, test_size=test_size, random_state=random_state, stratify=y_digits\n",
    "    )\n",
    "    X_val_digits, X_test_digits, y_val_digits, y_test_digits = train_test_split(\n",
    "        X_temp_digits, y_temp_digits, test_size=val_ratio,\n",
    "        random_state=random_state, stratify=y_temp_digits\n",
    "    )\n",
    "\n",
    "    # Split alpha data: 70% train, 15% val, 15% test\n",
    "    X_train_alphas, X_temp_alphas, y_train_alphas, y_temp_alphas = train_test_split(\n",
    "        X_alphas, y_alphas, test_size=test_size, random_state=random_state, stratify=y_alphas\n",
    "    )\n",
    "    X_val_alphas, X_test_alphas, y_val_alphas, y_test_alphas = train_test_split(\n",
    "        X_temp_alphas, y_temp_alphas, test_size=val_ratio,\n",
    "        random_state=random_state, stratify=y_temp_alphas\n",
    "    )\n",
    "\n",
    "    # Save digit data\n",
    "    np.save(f\"{output_dir}digits_X_train.npy\", X_train_digits)\n",
    "    np.save(f\"{output_dir}digits_y_train.npy\", y_train_digits)\n",
    "    np.save(f\"{output_dir}digits_X_val.npy\", X_val_digits)\n",
    "    np.save(f\"{output_dir}digits_y_val.npy\", y_val_digits)\n",
    "    np.save(f\"{output_dir}digits_X_test.npy\", X_test_digits)\n",
    "    np.save(f\"{output_dir}digits_y_test.npy\", y_test_digits)\n",
    "\n",
    "    # Save alpha data\n",
    "    np.save(f\"{output_dir}alphas_X_train.npy\", X_train_alphas)\n",
    "    np.save(f\"{output_dir}alphas_y_train.npy\", y_train_alphas)\n",
    "    np.save(f\"{output_dir}alphas_X_val.npy\", X_val_alphas)\n",
    "    np.save(f\"{output_dir}alphas_y_val.npy\", y_val_alphas)\n",
    "    np.save(f\"{output_dir}alphas_X_test.npy\", X_test_alphas)\n",
    "    np.save(f\"{output_dir}alphas_y_test.npy\", y_test_alphas)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DATASET SPLIT SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nDigits:\")\n",
    "    print(f\"  Training set:   {X_train_digits.shape[0]:6} samples ({X_train_digits.shape[0]/len(X_digits)*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {X_val_digits.shape[0]:6} samples ({X_val_digits.shape[0]/len(X_digits)*100:.1f}%)\")\n",
    "    print(f\"  Test set:       {X_test_digits.shape[0]:6} samples ({X_test_digits.shape[0]/len(X_digits)*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nAlphas:\")\n",
    "    print(f\"  Training set:   {X_train_alphas.shape[0]:6} samples ({X_train_alphas.shape[0]/len(X_alphas)*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {X_val_alphas.shape[0]:6} samples ({X_val_alphas.shape[0]/len(X_alphas)*100:.1f}%)\")\n",
    "    print(f\"  Test set:       {X_test_alphas.shape[0]:6} samples ({X_test_alphas.shape[0]/len(X_alphas)*100:.1f}%)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return {\n",
    "        'digits': {\n",
    "            'train': (X_train_digits, y_train_digits),\n",
    "            'val': (X_val_digits, y_val_digits),\n",
    "            'test': (X_test_digits, y_test_digits)\n",
    "        },\n",
    "        'alphas': {\n",
    "            'train': (X_train_alphas, y_train_alphas),\n",
    "            'val': (X_val_alphas, y_val_alphas),\n",
    "            'test': (X_test_alphas, y_test_alphas)\n",
    "        }\n",
    "    }\n",
    "\n",
    "split_data = split_and_save_data(X_digits, y_digits, X_alphas, y_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1762744704347,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "qVGleQeHqHqF",
    "outputId": "b2be3402-0028-4743-a5cd-825e7ba0a4b1"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Generator Class\n",
    "\n",
    "class Datasets:\n",
    "    \"\"\"Dataset loader and generator for train/val/test modes.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir=\"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/characters_thi_giac_may_tinh/char_bien_so\",\n",
    "                 mode='train', alpha_augmentation=8):\n",
    "        \"\"\"\n",
    "        Initialize dataset loader.\n",
    "\n",
    "        Parameters:\n",
    "            data_dir: Directory containing .npy files\n",
    "            mode: 'train', 'val', or 'test'\n",
    "            alpha_augmentation: Multiplier for alpha data (only applied in train mode)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.alpha_augmentation = alpha_augmentation if mode == 'train' else 1\n",
    "        self.all_data = []\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load digit and alpha data based on mode.\"\"\"\n",
    "        # Load digits\n",
    "        X_digits = np.load(f\"{self.data_dir}digits_X_{self.mode}.npy\")\n",
    "        y_digits = np.load(f\"{self.data_dir}digits_y_{self.mode}.npy\")\n",
    "        digits_data = list(zip(X_digits, y_digits))\n",
    "\n",
    "        # Load alphas\n",
    "        X_alphas = np.load(f\"{self.data_dir}alphas_X_{self.mode}.npy\")\n",
    "        y_alphas = np.load(f\"{self.data_dir}alphas_y_{self.mode}.npy\")\n",
    "        alphas_data = list(zip(X_alphas, y_alphas))\n",
    "\n",
    "        # Add digit data\n",
    "        self.all_data.extend(digits_data)\n",
    "\n",
    "        # Add alpha data with augmentation\n",
    "        for i in range(len(alphas_data) * self.alpha_augmentation):\n",
    "            self.all_data.append(alphas_data[i % len(alphas_data)])\n",
    "\n",
    "        print(f\"Loaded {self.mode} dataset:\")\n",
    "        print(f\"  Digits: {len(digits_data)} samples\")\n",
    "        print(f\"  Alphas: {len(alphas_data)} samples (x{self.alpha_augmentation} = {len(alphas_data) * self.alpha_augmentation})\")\n",
    "        print(f\"  Total: {len(self.all_data)} samples\")\n",
    "\n",
    "    def generate(self, shuffle=True, num_classes=32):\n",
    "        \"\"\"\n",
    "        Generate and return images and labels.\n",
    "\n",
    "        Parameters:\n",
    "            shuffle: Whether to shuffle data\n",
    "            num_classes: Number of classes for one-hot encoding\n",
    "\n",
    "        Returns:\n",
    "            images: numpy array of images\n",
    "            labels: one-hot encoded labels\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.all_data)\n",
    "\n",
    "        images = np.array([img for img, _ in self.all_data])\n",
    "        labels = np.array([label for _, label in self.all_data])\n",
    "        labels = keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def visualize_samples(self, n_samples=10, alpha_dict=ALPHA_DICT):\n",
    "        \"\"\"Visualize random samples from the dataset.\"\"\"\n",
    "        indices = np.random.choice(len(self.all_data), n_samples, replace=False)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            img, label = self.all_data[idx]\n",
    "            axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f\"Label: {alpha_dict[label]}\")\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.suptitle(f'{self.mode.capitalize()} Dataset Samples', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Dataset Generate\n",
    "train_dataset = Datasets(mode='train', alpha_augmentation=8)\n",
    "val_dataset = Datasets(mode='val')\n",
    "test_dataset = Datasets(mode='test')\n",
    "\n",
    "# Visualize samples\n",
    "train_dataset.visualize_samples(n_samples=10)\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = train_dataset.generate()\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1762744704628,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "HYN2DYI2qRS_",
    "outputId": "9edcfde0-5f43-4d0a-b9d7-33dfb38d932a"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Build CNN Model\n",
    "\n",
    "def build_cnn_model(input_shape=(28, 28, 1), num_classes=32, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Build and compile CNN model architecture.\n",
    "\n",
    "    Parameters:\n",
    "        input_shape: Shape of input images\n",
    "        num_classes: Number of output classes\n",
    "        learning_rate: Learning rate for optimizer\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        '''\n",
    "        First convolutional block\n",
    "        32 bộ lọc (filters)\n",
    "        kernel_size = 3x3\n",
    "        padding = 'same': giữ nguyên kích thước đầu ra bằng cách thêm viền bằng 0\n",
    "        activation = 'relu': f(x) = max(0, x)\n",
    "        MaxPooling2D(pool_size=(2, 2)): Lấy giá trị lớn nhất trong mỗi vùng 2x2 -> giảm kích thước ảnh 1/2\n",
    "        Dropout(0.25): Ngẫu nhiên bỏ 25% neuron\n",
    "        '''\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Third convolutional block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        '''\n",
    "        Fully connected layers\n",
    "        Flatten(): Làm phẳng tensor 3D -> 1D\n",
    "        Dense(512, activation='relu'): Fully connected layer với 512 neuron và hàm kích hoạt ReLU\n",
    "        Dense(32, activation='softmax'): Lớp đầu ra (32 lớp) <- softmax: tính xác suất cho từng lớp\n",
    "        '''\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax') # y_pred_i = exp(z_i) / Σ exp(z_j): j = 1 -> 32\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", # L = -Σ [y_true_i * log(y_pred_i)]: i = 1 -> 32\n",
    "        optimizer=optimizers.Adam(learning_rate),\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_cnn_model(input_shape=(28, 28, 1), num_classes=32)\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "Output_size = (Input_size - Kernel_size + 2 * Padding) / Stride + 1\n",
    "28x28x1 -> 28x28x32: 32 feature maps\n",
    "28x28x32 -> 26x26x32: 32 feature maps, không có padding -> giảm kích thước: (28-3+0)/1+1=26\n",
    "26x26x32 -> 13x13x32: pool_size 2x2 -> giảm kích thước 1/2\n",
    "13x13x32 -> 13x13x32: dropout 25% neurons\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1472494,
     "status": "ok",
     "timestamp": 1762746406437,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "pNc1tCW-qVJX",
    "outputId": "ead19927-f1ea-4bf7-8336-d525f9597bc4"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Training Function\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.metrics\")\n",
    "\n",
    "def calculate_f1_score(model, X, y_true_one_hot):\n",
    "    \"\"\"\n",
    "    Calculate macro F1 score for the model.\n",
    "\n",
    "    Parameters:\n",
    "        model: Keras model\n",
    "        X: Input data\n",
    "        y_true_one_hot: True labels (one-hot encoded)\n",
    "\n",
    "    Returns:\n",
    "        Macro F1 score\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true_one_hot, axis=1)\n",
    "    return f1_score(y_true_classes, y_pred_classes, average='macro', zero_division=1)\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, output_dir,\n",
    "                epochs=30, batch_size=128\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Train the CNN model with F1 score tracking.\n",
    "\n",
    "    Parameters:\n",
    "        model: Compiled Keras model\n",
    "        train_dataset: Training dataset (Datasets object)\n",
    "        val_dataset: Validation dataset (Datasets object)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        output_dir: Directory to save models and metrics\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    # Callbacks\n",
    "    # Giảm lr khi val_acc không cải thiện\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_acc',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f'{output_dir}weight.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Generate data\n",
    "    print(\"Generating training and validation data...\")\n",
    "    X_train, y_train = train_dataset.generate(shuffle=True)\n",
    "    X_val, y_val = val_dataset.generate(shuffle=False)\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_f1': [],\n",
    "        'val_f1': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Train for one epoch\n",
    "        h = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[checkpoint, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Extract metrics\n",
    "        train_acc = h.history['acc'][0]\n",
    "        val_acc = h.history['val_acc'][0]\n",
    "        train_loss = h.history['loss'][0]\n",
    "        val_loss = h.history['val_loss'][0]\n",
    "\n",
    "        # Calculate F1 scores\n",
    "        train_f1 = calculate_f1_score(model, X_train, y_train)\n",
    "        val_f1 = calculate_f1_score(model, X_val, y_val)\n",
    "\n",
    "        # Store metrics\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save final model\n",
    "    model.save(f'{output_dir}')\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training completed! Model saved as {output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return history\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/model/character_recognition.keras\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27438,
     "status": "ok",
     "timestamp": 1762746433873,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "CLmwAgs_qY_v",
    "outputId": "fc1ce73b-b978-4cb3-ad7c-cdfb725475fc"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Model Evaluation\n",
    "\n",
    "def evaluate_model(model, train_dataset, val_dataset, test_dataset,\n",
    "                   alpha_dict=ALPHA_DICT):\n",
    "    \"\"\"\n",
    "    Evaluate model on all datasets and print detailed reports.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "        test_dataset: Test dataset\n",
    "        alpha_dict: Dictionary mapping label indices to characters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Generate data\n",
    "    X_train, y_train = train_dataset.generate(shuffle=False)\n",
    "    X_val, y_val = val_dataset.generate(shuffle=False)\n",
    "    X_test, y_test = test_dataset.generate(shuffle=False)\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"FINAL MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Evaluate on each dataset\n",
    "    for name, X, y in [('Train', X_train, y_train),\n",
    "                        ('Validation', X_val, y_val),\n",
    "                        ('Test', X_test, y_test)]:\n",
    "\n",
    "        # Calculate metrics\n",
    "        loss, acc = model.evaluate(X, y, verbose=0)\n",
    "        f1 = calculate_f1_score(model, X, y)\n",
    "\n",
    "        results[name.lower()] = {\n",
    "            'loss': loss,\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "\n",
    "        print(f\"\\n{name} Set:\")\n",
    "        print(f\"  Loss:     {loss:.4f}\")\n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Detailed classification reports\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLASSIFICATION REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for name, X, y in [('Validation', X_val, y_val), ('Test', X_test, y_test)]:\n",
    "        y_pred = model.predict(X, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "        print(f\"\\n{name} Set Classification Report:\")\n",
    "        print(\"-\" * 70)\n",
    "        # Only show classes 0-30 (exclude background)\n",
    "        target_names = [alpha_dict[i] for i in range(31)]\n",
    "        print(classification_report(\n",
    "            y_true_classes,\n",
    "            y_pred_classes,\n",
    "            target_names=target_names,\n",
    "            zero_division=1\n",
    "        ))\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, alpha_dict=ALPHA_DICT\n",
    "                          # output_path=\"/content/drive/MyDrive/LicensePlateRecognition/visualization/confusion_matrix.png\"\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for a dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model\n",
    "        dataset: Dataset to evaluate\n",
    "        alpha_dict: Dictionary mapping label indices to characters\n",
    "        output_path: Path to save the confusion matrix plot\n",
    "    \"\"\"\n",
    "    X, y = dataset.generate(shuffle=False)\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "    # Compute confusion matrix (exclude background class 31)\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes, labels=range(31))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[alpha_dict[i] for i in range(31)],\n",
    "                yticklabels=[alpha_dict[i] for i in range(31)],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(output_path, dpi=150)\n",
    "    plt.show()\n",
    "    # print(f\"Confusion matrix saved to: {output_path}\")\n",
    "\n",
    "results = evaluate_model(model, train_dataset, val_dataset, test_dataset)\n",
    "plot_confusion_matrix(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3819,
     "status": "ok",
     "timestamp": 1762746437693,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "GbP9kx8ZqcM3",
    "outputId": "4a3f2636-9652-4d2c-f332-f129f2d7e6b1"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Visualization Functions\n",
    "\n",
    "def plot_training_curves(history, test_acc=None, test_f1=None,\n",
    "                         output_dir=\"/content/drive/MyDrive/LicensePlateRecognition/visualization/\"):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training curves including accuracy, F1 score, and loss.\n",
    "\n",
    "    Parameters:\n",
    "        history: Training history dictionary from train_model()\n",
    "        test_acc: Test accuracy (optional)\n",
    "        test_f1: Test F1 score (optional)\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "\n",
    "    # Create comprehensive plot\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # 1. Accuracy curve\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    ax1.plot(epochs, history['train_acc'], 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "    ax1.plot(epochs, history['val_acc'], 'r-s', label='Validation', linewidth=2, markersize=4)\n",
    "    if test_acc is not None:\n",
    "        ax1.axhline(y=test_acc, color='g', linestyle='--', linewidth=2,\n",
    "                    label=f'Test: {test_acc:.4f}')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. F1 Score curve\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    ax2.plot(epochs, history['train_f1'], 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "    ax2.plot(epochs, history['val_f1'], 'r-s', label='Validation', linewidth=2, markersize=4)\n",
    "    if test_f1 is not None:\n",
    "        ax2.axhline(y=test_f1, color='g', linestyle='--', linewidth=2,\n",
    "                    label=f'Test: {test_f1:.4f}')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax2.set_title('Model F1 Score', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Loss curve\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.plot(epochs, history['train_loss'], 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "    ax3.plot(epochs, history['val_loss'], 'r-s', label='Validation', linewidth=2, markersize=4)\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Loss', fontsize=12)\n",
    "    ax3.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Final comparison bar chart\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    metrics = ['Accuracy', 'F1 Score']\n",
    "    train_final = [history['train_acc'][-1], history['train_f1'][-1]]\n",
    "    val_final = [history['val_acc'][-1], history['val_f1'][-1]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "\n",
    "    ax4.bar(x - width, train_final, width, label='Train', color='blue', alpha=0.8)\n",
    "    ax4.bar(x, val_final, width, label='Validation', color='red', alpha=0.8)\n",
    "\n",
    "    if test_acc is not None and test_f1 is not None:\n",
    "        test_final = [test_acc, test_f1]\n",
    "        ax4.bar(x + width, test_final, width, label='Test', color='green', alpha=0.8)\n",
    "\n",
    "    ax4.set_ylabel('Score', fontsize=12)\n",
    "    ax4.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(metrics)\n",
    "    ax4.legend(fontsize=10)\n",
    "    ax4.grid(True, axis='y', alpha=0.3)\n",
    "    ax4.set_ylim([0, 1.05])\n",
    "\n",
    "    # 5. Accuracy improvement over epochs\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    train_improvement = np.array(history['train_acc']) - history['train_acc'][0]\n",
    "    val_improvement = np.array(history['val_acc']) - history['val_acc'][0]\n",
    "    ax5.plot(epochs, train_improvement, 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "    ax5.plot(epochs, val_improvement, 'r-s', label='Validation', linewidth=2, markersize=4)\n",
    "    ax5.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax5.set_xlabel('Epoch', fontsize=12)\n",
    "    ax5.set_ylabel('Improvement', fontsize=12)\n",
    "    ax5.set_title('Accuracy Improvement from Epoch 1', fontsize=14, fontweight='bold')\n",
    "    ax5.legend(fontsize=10)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Training summary statistics\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "\n",
    "    summary_text = f\"\"\"\n",
    "    TRAINING SUMMARY\n",
    "    {'='*40}\n",
    "\n",
    "    Best Training Accuracy:    {max(history['train_acc']):.4f}\n",
    "    Best Validation Accuracy:  {max(history['val_acc']):.4f}\n",
    "    Best Training F1:          {max(history['train_f1']):.4f}\n",
    "    Best Validation F1:        {max(history['val_f1']):.4f}\n",
    "\n",
    "    Final Training Accuracy:   {history['train_acc'][-1]:.4f}\n",
    "    Final Validation Accuracy: {history['val_acc'][-1]:.4f}\n",
    "    Final Training F1:         {history['train_f1'][-1]:.4f}\n",
    "    Final Validation F1:       {history['val_f1'][-1]:.4f}\n",
    "\n",
    "    Total Epochs:              {len(epochs)}\n",
    "    \"\"\"\n",
    "\n",
    "    if test_acc is not None and test_f1 is not None:\n",
    "        summary_text += f\"\"\"\n",
    "    Test Accuracy:             {test_acc:.4f}\n",
    "    Test F1 Score:             {test_f1:.4f}\n",
    "        \"\"\"\n",
    "\n",
    "    ax6.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "             verticalalignment='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'{output_dir}training_curves_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # print(f\"Comprehensive training curves saved to: {output_dir}training_curves_comprehensive.png\")\n",
    "\n",
    "def plot_sample_predictions(model, dataset, alpha_dict=ALPHA_DICT,\n",
    "                           n_samples=20,\n",
    "                            output_path=\"/content/drive/MyDrive/LicensePlateRecognition/visualization/predictions.png\"):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on random samples.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model\n",
    "        dataset: Dataset to sample from\n",
    "        alpha_dict: Dictionary mapping indices to labels\n",
    "        n_samples: Number of samples to display\n",
    "        output_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    X, y = dataset.generate(shuffle=True)\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "\n",
    "    predictions = model.predict(X[indices], verbose=0)\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y[indices], axis=1)\n",
    "\n",
    "    rows = 4\n",
    "    cols = 5\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        axes[i].imshow(X[indices[i]].squeeze(), cmap='gray')\n",
    "\n",
    "        true_label = alpha_dict[true_classes[i]]\n",
    "        pred_label = alpha_dict[pred_classes[i]]\n",
    "        confidence = predictions[i][pred_classes[i]]\n",
    "\n",
    "        color = 'green' if true_classes[i] == pred_classes[i] else 'red'\n",
    "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.2f})',\n",
    "                         color=color, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle('Model Predictions (Green=Correct, Red=Incorrect)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # print(f\"Sample predictions saved to: {output_path}\")\n",
    "\n",
    "plot_training_curves(history, test_acc=results['test']['accuracy'],\n",
    "                    test_f1=results['test']['f1_score'])\n",
    "plot_sample_predictions(model, test_dataset, n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18943,
     "status": "ok",
     "timestamp": 1762746599659,
     "user": {
      "displayName": "B22DCKH088_Lê Đăng Phúc",
      "userId": "14973214158681730128"
     },
     "user_tz": -420
    },
    "id": "wSjr_Cj3qhc_",
    "outputId": "f2cd3ba7-9286-4996-84df-0bf2d4a92780"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Inference and Prediction Utilities\n",
    "\n",
    "def load_trained_model(model_path=\"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/model/character_recognition.keras\"):\n",
    "    \"\"\"\n",
    "    Load a trained model from file.\n",
    "\n",
    "    Parameters:\n",
    "        model_path: Path to saved model\n",
    "\n",
    "    Returns:\n",
    "        Loaded Keras model\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Model loaded successfully from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path, image_size=28):\n",
    "    \"\"\"\n",
    "    Preprocess a single image for prediction.\n",
    "\n",
    "    Parameters:\n",
    "        image_path: Path to image file\n",
    "        image_size: Target size for resizing\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image ready for prediction\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (image_size, image_size), cv2.INTER_AREA)\n",
    "    img = img.reshape((1, image_size, image_size, 1))\n",
    "    return img\n",
    "\n",
    "def predict_character(model, image_path, alpha_dict=ALPHA_DICT,\n",
    "                     image_size=28, top_k=3):\n",
    "    \"\"\"\n",
    "    Predict character from an image file.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model\n",
    "        image_path: Path to image file\n",
    "        alpha_dict: Dictionary mapping indices to characters\n",
    "        image_size: Image size for preprocessing\n",
    "        top_k: Number of top predictions to return\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with predictions and confidences\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    img = preprocess_image(image_path, image_size)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(img, verbose=0)[0]\n",
    "\n",
    "    # Get top-k predictions\n",
    "    top_indices = np.argsort(prediction)[-top_k:][::-1]\n",
    "    top_probs = prediction[top_indices]\n",
    "    top_chars = [alpha_dict[idx] for idx in top_indices]\n",
    "\n",
    "    result = {\n",
    "        'top_prediction': top_chars[0],\n",
    "        'confidence': float(top_probs[0]),\n",
    "        'top_k_predictions': list(zip(top_chars, [float(p) for p in top_probs]))\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def predict_batch(model, image_paths, alpha_dict=ALPHA_DICT, image_size=28):\n",
    "    \"\"\"\n",
    "    Predict characters for multiple images.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model\n",
    "        image_paths: List of image paths\n",
    "        alpha_dict: Dictionary mapping indices to characters\n",
    "        image_size: Image size for preprocessing\n",
    "\n",
    "    Returns:\n",
    "        List of prediction results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        result = predict_character(model, img_path, alpha_dict, image_size)\n",
    "        results.append({\n",
    "            'image_path': img_path,\n",
    "            'prediction': result['top_prediction'],\n",
    "            'confidence': result['confidence']\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def visualize_prediction(model, image_path, alpha_dict=ALPHA_DICT,\n",
    "                        image_size=28, top_k=5):\n",
    "    \"\"\"\n",
    "    Visualize prediction with confidence scores.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model\n",
    "        image_path: Path to image file\n",
    "        alpha_dict: Dictionary mapping indices to characters\n",
    "        image_size: Image size for preprocessing\n",
    "        top_k: Number of top predictions to show\n",
    "    \"\"\"\n",
    "    # Preprocess and predict\n",
    "    img = preprocess_image(image_path, image_size)\n",
    "    prediction = model.predict(img, verbose=0)[0]\n",
    "\n",
    "    # Get top-k predictions\n",
    "    top_indices = np.argsort(prediction)[-top_k:][::-1]\n",
    "    top_probs = prediction[top_indices]\n",
    "    top_chars = [alpha_dict[idx] for idx in top_indices]\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Display image\n",
    "    original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    ax1.imshow(original_img, cmap='gray')\n",
    "    ax1.set_title(f'Input Image\\nPredicted: {top_chars[0]} ({top_probs[0]:.2%})',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Display confidence bars\n",
    "    colors = ['green' if i == 0 else 'skyblue' for i in range(top_k)]\n",
    "    bars = ax2.barh(range(top_k), top_probs, color=colors)\n",
    "    ax2.set_yticks(range(top_k))\n",
    "    ax2.set_yticklabels(top_chars)\n",
    "    ax2.set_xlabel('Confidence', fontsize=11)\n",
    "    ax2.set_title(f'Top {top_k} Predictions', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    # Add percentage labels\n",
    "    for i, (bar, prob) in enumerate(zip(bars, top_probs)):\n",
    "        ax2.text(prob + 0.02, i, f'{prob:.2%}',\n",
    "                va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nPrediction Results for: {image_path}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (char, prob) in enumerate(zip(top_chars, top_probs), 1):\n",
    "        print(f\"{i}. Character: {char:3s} | Confidence: {prob:.4f} ({prob:.2%})\")\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = load_trained_model(\"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/model/character_recognition.keras\")\n",
    "\n",
    "\n",
    "# Batch prediction\n",
    "import glob\n",
    "IMAGE_DIR = \"/content/drive/MyDrive/Colab Notebooks/DigitalImageProcessing/LicensePlateRecognition/data/manual-test/characters\"\n",
    "search_pattern = os.path.join(IMAGE_DIR, '**', '*.[jp][pn][eg]*')\n",
    "image_paths = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "for img_path in image_paths:\n",
    "  visualize_prediction(model, img_path, top_k=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
